{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-fold classification code for BraTS2019 dataset (RGBA images - 4 channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn.metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==1.0.61 --no-deps\n",
    "# !pip install torch==1.4 torchvision==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set batch size according to useable memory and imsize according to model needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai; fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "imsize = 96\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets, **kwargs):\n",
    "        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-CE_loss)\n",
    "        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n",
    "        return F_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_result(learn):\n",
    "    interp = ClassificationInterpretation.from_learner(learn)\n",
    "    interp.plot_confusion_matrix()\n",
    "#     plt.savefig('confusion_matrix.png')\n",
    "    \n",
    "    preds,y, loss = learn.get_preds(with_loss=True)\n",
    "\n",
    "    # get accuracy\n",
    "    acc = accuracy(preds, y)\n",
    "    print('Validation set: the accuracy is {0}.'.format(acc))\n",
    "\n",
    "    # F1 score\n",
    "    pred_valid = np.argmax(preds, axis=1)\n",
    "    \n",
    "    F1_validation=f1_score(pred_valid, y,average='weighted')\n",
    "    print('Validation set: F1_score is {0}.'.format(F1_validation)) \n",
    "    \n",
    "    pred_id, lables_pred = torch.max(preds,dim=1)\n",
    "    lables_true=y\n",
    "    \n",
    "    lables_true_array = y.numpy() # fron tensot to array\n",
    "    lables_pred_array=lables_pred.numpy()\n",
    "    \n",
    "    lables_true_array = y.numpy() # fron tensot to array\n",
    "    lables_pred_array=lables_pred.numpy()\n",
    "    \n",
    "    # # Majority of Votes:\n",
    "\n",
    "    MV_labels = [];                                          # majority of votes decision list\n",
    "    MV_predicted_labels = []\n",
    "    for i in range(0, len(lables_pred_array),5):               \n",
    "        curr_patient = lables_pred_array[i:i+5]              # running on each patient\n",
    "        occurances = Counter(curr_patient)                   # finding how many times each prediction was appeared\n",
    "        max_key = max(occurances, key = occurances.get)      # extracting the most common appearance\n",
    "        MV_labels.append(max_key) \n",
    "    #     print(\"for patient \"+str(i)+' the *predicted* diagnosos is '+str(max_key))\n",
    "        MV_predicted_labels.append(max_key)\n",
    "\n",
    "    MV_True_labels = [];                                     # majority of votes decision list\n",
    "    for i in range(0, len(lables_true_array),5):               \n",
    "        curr_patient = lables_true_array[i:i+5]              # running on each patient\n",
    "        occurances = Counter(curr_patient)                   # finding how many times each prediction was appeared\n",
    "        max_key = max(occurances, key = occurances.get)      # extracting the most common appearance\n",
    "        MV_True_labels.append(max_key) \n",
    "    #     print(\"for patient \"+str(i)+' the *True* diagnosos is '+str(max_key))\n",
    "\n",
    "    lst=[]\n",
    "    for i in range(len(MV_True_labels)):    \n",
    "        if MV_True_labels[i] == MV_labels[i]:  \n",
    "    #         print(\"patient\" +str(i+1)+\" TRUE prediction\")\n",
    "            lst.append(\"TRUE\")\n",
    "        else:  \n",
    "    #         print(\"patient\" +str(i+1)+\" FALSE prediction\")\n",
    "            lst.append(\"FALSE\")\n",
    "        \n",
    "    # Accuracy after majority of votes:\n",
    "    acc_MV = lst.count(\"TRUE\")/len(MV_True_labels)\n",
    "    print('Validation set: Accuracy after majority of votes is {0}.' .format(acc_MV))\n",
    "    # F1 score\n",
    "    F1_validation=f1_score(MV_predicted_labels, MV_True_labels, average='weighted')\n",
    "    print('Validation set: F1_score after majority of votes is {0}.'.format(F1_validation))\n",
    "    \n",
    "    \n",
    "    # Confusion Matrix after majority of votes:\n",
    "    cf = confusion_matrix(MV_predicted_labels, MV_True_labels)\n",
    "    cf\n",
    "    # disp = ConfusionMatrixDisplay(cf,display_labels=classes)\n",
    "    # disp.plot(xticks_rotation='vertical',cmap=plt.cm.Blues)\n",
    "    # plt.title('Confusion Matrix - validation set after majority of votes')\n",
    "    # plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_model(*args, **kwargs):\n",
    "   \n",
    "    model = models.resnet152(*args, **kwargs)\n",
    "    model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = #BLINDED\n",
    "path_images = Path(path + '/TRAIN_images')\n",
    "\n",
    "epochs = 2\n",
    "lr=1e-02 # 0.01\n",
    "loss_func = FocalLoss(alpha=0.25, gamma=2) # best - .81 - (alpha=0.2, gamma=3)\n",
    "\n",
    "for i_fold in range(1,6):\n",
    "    \n",
    "    df=pd.read_csv(path+f'/TRAIN_FOLD{i_fold}.csv') \n",
    "\n",
    "    IS_VALID=\"Val\"\n",
    "\n",
    "    # Creating augmentation\n",
    "    tfms = get_transforms(do_flip=True,flip_vert=True, max_rotate=2.0, max_lighting=0.5, max_zoom=0)\n",
    "\n",
    "    data = (ImageList.from_df(df, path_images, suffix='.TIFF',convert_mode='RGBA')\n",
    "           .split_from_df(IS_VALID)\n",
    "           .label_from_df()\n",
    "           .transform(tfms, size=imsize)\n",
    "           .databunch(bs=bs))\n",
    "    \n",
    "    \n",
    "    print(f'Training fold {i_fold}:')\n",
    "    learn = cnn_learner(data, new_model, metrics=[accuracy],callback_fns=[callbacks.OverSamplingCallback],loss_func=loss_func, wd=0.001,ps=0.5)\n",
    "    learn.fit_one_cycle(epochs, max_lr=slice(lr/3,lr), callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='accuracy',name='best')]) # save the best mode\n",
    "    view_result(learn)\n",
    "          \n",
    "    learn.save(f\"Classification_model_fold{i_fold}\")\n",
    "    learn.export(f'models/Classification_model{i_fold}.pkl')\n",
    "    print('//////////////////////////////')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
